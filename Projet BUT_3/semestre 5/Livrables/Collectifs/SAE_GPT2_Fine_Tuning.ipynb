{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL6DRTkj4VqE"
      },
      "source": [
        "#GPT-2 Setup & Fine Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDq3dZw34IgX"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Before we start implementing GPT-2, we install and import all the\n",
        "libraries we need.\n",
        "\n",
        "In this notebook, wse will be using the KerasNLP library.\n",
        "\n",
        "We will enable mixed precision training to save training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeijUgRu36qf",
        "outputId": "cae28388-a575-4e2e-96b4-985487713370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-nlp\n",
            "  Downloading keras_nlp-0.7.0-py3-none-any.whl (415 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.4/415.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-core (from keras-nlp)\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (23.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (2023.6.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (13.7.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (0.1.8)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (0.1.5)\n",
            "Collecting tensorflow-text (from keras-nlp)\n",
            "  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-nlp) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-nlp) (4.66.1)\n",
            "Collecting namex (from keras-core->keras-nlp)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-nlp) (3.9.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nlp) (2.16.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras-nlp) (0.15.0)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras-nlp) (2.15.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (2.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-nlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-nlp) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-nlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-nlp) (2023.11.17)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (3.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras-nlp) (3.2.2)\n",
            "Installing collected packages: namex, keras-core, tensorflow-text, keras-nlp\n",
            "Successfully installed keras-core-0.1.7 keras-nlp-0.7.0 namex-0.0.7 tensorflow-text-2.15.0\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.6)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-nlp\n",
        "!pip install pandas\n",
        "!pip install opendatasets\n",
        "!pip install pyyaml h5py  # Required to save models in HDF5 format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2b1MyzDF4d5B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f91c246-2739-4fd3-a577-894362d9a193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ],
      "source": [
        "import keras_nlp\n",
        "import pandas as pd\n",
        "import opendatasets as od\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "policy = keras.mixed_precision.Policy(\"mixed_float16\")\n",
        "keras.mixed_precision.set_global_policy(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvUwU2oA5BXE"
      },
      "source": [
        "Set the Keras Backend to tensorflow (can also use \"jax\" or \"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aWVrfMo547fT"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvO-U_5h4tbb"
      },
      "source": [
        "Definition of some hyper-parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OnxSbnP94v_O"
      },
      "outputs": [],
      "source": [
        "# General hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "NUM_BATCHES = 500\n",
        "EPOCHS = 1\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "MAX_GENERATION_LENGTH = 200\n",
        "SUBSET_SIZE = 1250\n",
        "\n",
        "GPT2_PRESET = \"gpt2_large_en\"\n",
        "\n",
        "# LoRA-specific hyperparameters\n",
        "RANK = 4\n",
        "ALPHA = 32.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXJemr-S5ZpD"
      },
      "source": [
        "## Fine tuning\n",
        "\n",
        "Put \"medsquad.csv\" in the same folder as this notebook\n",
        "\n",
        "(Download link: https://www.kaggle.com/datasets/jpmiller/layoutlm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi9wD-tpMdaC",
        "outputId": "0fec980a-8226-467e-c37c-c2838b15038d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: tesr33\n",
            "Your Kaggle Key: ··········\n",
            "Downloading layoutlm.zip to ./layoutlm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8.16G/8.16G [01:28<00:00, 98.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/jpmiller/layoutlm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHOu7YGW5qUO"
      },
      "source": [
        "Read the csv file and removes empty columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wSSa-S895p_h"
      },
      "outputs": [],
      "source": [
        "health_df = pd.read_csv(\"./layoutlm/medquad.csv\")\n",
        "health_df = health_df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rI8G9CE5xgl"
      },
      "source": [
        "Transform Pandas dataframe into a tensorflow Tensor object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "coGHLS3JzOKb"
      },
      "outputs": [],
      "source": [
        "def split_dataframe_to_datasets(df, nb, subset_size=SUBSET_SIZE):\n",
        "    # Get the total number of rows in the DataFrame\n",
        "    total_rows = len(df)\n",
        "\n",
        "    # Calculate the number of subsets needed\n",
        "    num_subsets = (total_rows + subset_size - 1) // subset_size\n",
        "\n",
        "    # Initialize an empty list to store subsets\n",
        "    subsets = []\n",
        "\n",
        "    if nb == 1:\n",
        "      s,e = 0,num_subsets//2\n",
        "\n",
        "    if nb == 2:\n",
        "      s,e = num_subsets//2,num_subsets\n",
        "\n",
        "    # Loop through and create subsets\n",
        "    for i in range(s,e):\n",
        "        start_idx = i * subset_size\n",
        "        end_idx = min((i + 1) * subset_size, total_rows)\n",
        "        subset = df.iloc[start_idx:end_idx]\n",
        "        subsets.append(tf.data.Dataset.from_tensor_slices(subset))\n",
        "\n",
        "    return subsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SteGDXnv5w_F"
      },
      "outputs": [],
      "source": [
        "health_ds = split_dataframe_to_datasets(health_df,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Function for Generating Text with Time Tracking"
      ],
      "metadata": {
        "id": "a4KlRsdG0DSd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8d94MPOD-cex"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, input_text, max_length=200):\n",
        "    start = time.time()\n",
        "\n",
        "    output = model.generate(input_text, max_length=max_length)\n",
        "    print(\"\\nOutput:\")\n",
        "    print(output)\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"Total Time Elapsed: {end - start:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer and Loss Configuration for Neural Network Training with AdamW and Sparse Categorical Crossentropy"
      ],
      "metadata": {
        "id": "n-R_-D7g01hM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "b8SLqO3B-eE4"
      },
      "outputs": [],
      "source": [
        "def get_optimizer_and_loss():\n",
        "    optimizer = keras.optimizers.AdamW(\n",
        "        learning_rate=5e-5,\n",
        "        weight_decay=0.01,\n",
        "        epsilon=1e-6,\n",
        "        global_clipnorm=1.0,  # Gradient clipping.\n",
        "    )\n",
        "    # Exclude layernorm and bias terms from weight decay.\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"bias\"])\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"gamma\"])\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"beta\"])\n",
        "\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    return optimizer, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enhancing Dense Layers with Low-Rank Addition for Model Regularization"
      ],
      "metadata": {
        "id": "h33M_5nH0mC8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "63s2zYXm-gEr"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "class LoraLayer(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        original_layer,\n",
        "        rank=8,\n",
        "        alpha=32,\n",
        "        trainable=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # We want to keep the name of this layer the same as the original\n",
        "        # dense layer.\n",
        "        original_layer_config = original_layer.get_config()\n",
        "        name = original_layer_config[\"name\"]\n",
        "\n",
        "        kwargs.pop(\"name\", None)\n",
        "\n",
        "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
        "\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self._scale = alpha / rank\n",
        "\n",
        "        self._num_heads = original_layer_config[\"output_shape\"][-2]\n",
        "        self._hidden_dim = self._num_heads * original_layer_config[\"output_shape\"][-1]\n",
        "\n",
        "        # Layers.\n",
        "\n",
        "        # Original dense layer.\n",
        "        self.original_layer = original_layer\n",
        "        # No matter whether we are training the model or are in inference mode,\n",
        "        # this layer should be frozen.\n",
        "        self.original_layer.trainable = False\n",
        "\n",
        "        # LoRA dense layers.\n",
        "        self.A = keras.layers.Dense(\n",
        "            units=rank,\n",
        "            use_bias=False,\n",
        "            # Note: the original paper mentions that normal distribution was\n",
        "            # used for initialization. However, the official LoRA implementation\n",
        "            # uses \"Kaiming/He Initialization\".\n",
        "            kernel_initializer=keras.initializers.VarianceScaling(\n",
        "                scale=math.sqrt(5), mode=\"fan_in\", distribution=\"uniform\"\n",
        "            ),\n",
        "            trainable=trainable,\n",
        "            name=f\"lora_A\",\n",
        "        )\n",
        "        # B has the same `equation` and `output_shape` as the original layer.\n",
        "        # `equation = abc,cde->abde`, where `a`: batch size, `b`: sequence\n",
        "        # length, `c`: `hidden_dim`, `d`: `num_heads`,\n",
        "        # `e`: `hidden_dim//num_heads`. The only difference is that in layer `B`,\n",
        "        # `c` represents `rank`.\n",
        "        self.B = keras.layers.EinsumDense(\n",
        "            equation=original_layer_config[\"equation\"],\n",
        "            output_shape=original_layer_config[\"output_shape\"],\n",
        "            kernel_initializer=\"zeros\",\n",
        "            trainable=trainable,\n",
        "            name=f\"lora_B\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        original_output = self.original_layer(inputs)\n",
        "        if self.trainable:\n",
        "            # If we are fine-tuning the model, we will add LoRA layers' output\n",
        "            # to the original layer's output.\n",
        "            lora_output = self.B(self.A(inputs)) * self._scale\n",
        "            return original_output + lora_output\n",
        "\n",
        "        # If we are in inference mode, we \"merge\" the LoRA layers' weights into\n",
        "        # the original layer's weights - more on this in the text generation\n",
        "        # section!\n",
        "        return original_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memory Management and Model Loading: Resetting GPU Memory Stats and Initializing a GPT-2 Causal Language Model"
      ],
      "metadata": {
        "id": "En2W1o2h1SO8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TR7debff-hMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1ab178-80a2-48d4-8e86-6b444bab2010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_large_en/2/download/tokenizer.json...\n",
            "100%|██████████| 448/448 [00:00<00:00, 323kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_large_en/2/download/assets/tokenizer/merges.txt...\n",
            "100%|██████████| 446k/446k [00:00<00:00, 1.79MB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_large_en/2/download/assets/tokenizer/vocabulary.json...\n",
            "100%|██████████| 0.99M/0.99M [00:00<00:00, 2.97MB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_large_en/2/download/config.json...\n",
            "100%|██████████| 485/485 [00:00<00:00, 182kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_large_en/2/download/model.weights.h5...\n",
            "100%|██████████| 2.88G/2.88G [01:31<00:00, 33.7MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:37: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:37: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n"
          ]
        }
      ],
      "source": [
        "# This resets \"peak\" memory usage to \"current\" memory usage.\n",
        "tf.config.experimental.reset_memory_stats(\"GPU:0\")\n",
        "\n",
        "# Load the original model.\n",
        "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    GPT2_PRESET,\n",
        "    sequence_length=128,\n",
        ")\n",
        "lora_model = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "    GPT2_PRESET,\n",
        "    preprocessor=preprocessor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lora Modification of Self-Attention Layers in GPT-2 Language Model"
      ],
      "metadata": {
        "id": "-Wcz6f9525SE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5S4VzEZU_FWg"
      },
      "outputs": [],
      "source": [
        "for layer_idx in range(lora_model.backbone.num_layers):\n",
        "    # Change query dense layer.\n",
        "    decoder_layer = lora_model.backbone.get_layer(f\"transformer_layer_{layer_idx}\")\n",
        "    self_attention_layer = decoder_layer._self_attention_layer\n",
        "\n",
        "    # Change query dense layer.\n",
        "    self_attention_layer._query_dense = LoraLayer(\n",
        "        self_attention_layer._query_dense,\n",
        "        rank=RANK,\n",
        "        alpha=ALPHA,\n",
        "        trainable=True,\n",
        "    )\n",
        "\n",
        "    # Change value dense layer.\n",
        "    self_attention_layer._value_dense = LoraLayer(\n",
        "        self_attention_layer._value_dense,\n",
        "        rank=RANK,\n",
        "        alpha=ALPHA,\n",
        "        trainable=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2 Model Inference with a Sample Text Sequence"
      ],
      "metadata": {
        "id": "IAd8z0Ff3N21"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CEG8l_3O_K4L"
      },
      "outputs": [],
      "source": [
        "lora_model(preprocessor([\"LoRA is very useful for quick LLM finetuning\"])[0])\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning Configuration: Setting Trainable State for LoRA Layers in the GPT-2 Model"
      ],
      "metadata": {
        "id": "9pSnFX4e3Q1y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6qS8NXMQ_L9F"
      },
      "outputs": [],
      "source": [
        "for layer in lora_model._flatten_layers():\n",
        "    lst_of_sublayers = list(layer._flatten_layers())\n",
        "\n",
        "    if len(lst_of_sublayers) == 1:  # \"leaves of the model\"\n",
        "        if layer.name in [\"lora_A\", \"lora_B\"]:\n",
        "            layer.trainable = True\n",
        "        else:\n",
        "            layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7sibkmKv_NFw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "ff08d2ac-d9c3-488c-b5e1-9150c443643f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gpt2_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gpt2_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gpt2_tokenizer (\u001b[38;5;33mGPT2Tokenizer\u001b[0m)                     │                                              \u001b[38;5;34m50,257\u001b[0m │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gpt2_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Tokenizer</span>)                     │                                              <span style=\"color: #00af00; text-decoration-color: #00af00\">50,257</span> │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gpt2_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt2_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ gpt2_backbone (\u001b[38;5;33mGPT2Backbone\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                     │     \u001b[38;5;34m774,767,360\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_embedding (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50257\u001b[0m)                    │      \u001b[38;5;34m64,328,960\u001b[0m │\n",
              "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                  </span>┃<span style=\"font-weight: bold\"> Output Shape                           </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ gpt2_backbone (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Backbone</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">774,767,360</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50257</span>)                    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">64,328,960</span> │\n",
              "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m774,767,360\u001b[0m (2.89 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">774,767,360</span> (2.89 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m737,280\u001b[0m (2.81 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">737,280</span> (2.81 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m774,030,080\u001b[0m (2.88 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">774,030,080</span> (2.88 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "lora_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training GPT-2 Model with Checkpointing and Saving Weights to Google Drive"
      ],
      "metadata": {
        "id": "Gx0uQM4z38vj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bu19rcU5EBCr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dd0d325-d014-441b-ecbb-ef1b57c044df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training_1/cp.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/task.py:47: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/task.py:47: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1250/1250 [==============================] - 677s 406ms/step - loss: 0.5247 - accuracy: 0.1578\n",
            "Epoch 2/3\n",
            "1250/1250 [==============================] - 502s 402ms/step - loss: 0.4601 - accuracy: 0.1682\n",
            "Epoch 3/3\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4341 - accuracy: 0.1729\n",
            "Epoch 3: saving model to training_1/cp.ckpt\n",
            "1250/1250 [==============================] - 523s 418ms/step - loss: 0.4341 - accuracy: 0.1730\n",
            "Epoch 1/3\n",
            "1250/1250 [==============================] - 499s 399ms/step - loss: 0.5315 - accuracy: 0.1689\n",
            "Epoch 2/3\n",
            "1250/1250 [==============================] - 497s 398ms/step - loss: 0.5094 - accuracy: 0.1716\n",
            "Epoch 3/3\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4955 - accuracy: 0.1741\n",
            "Epoch 3: saving model to training_1/cp.ckpt\n",
            "1250/1250 [==============================] - 602s 481ms/step - loss: 0.4956 - accuracy: 0.1741\n",
            "Epoch 1/3\n",
            "1250/1250 [==============================] - 500s 400ms/step - loss: 0.5610 - accuracy: 0.1733\n",
            "Epoch 2/3\n",
            "1250/1250 [==============================] - 508s 406ms/step - loss: 0.5162 - accuracy: 0.1801\n",
            "Epoch 3/3\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4977 - accuracy: 0.1827\n",
            "Epoch 3: saving model to training_1/cp.ckpt\n",
            "1250/1250 [==============================] - 579s 463ms/step - loss: 0.4978 - accuracy: 0.1826\n",
            "Epoch 1/3\n",
            "1250/1250 [==============================] - 509s 407ms/step - loss: 0.5186 - accuracy: 0.1797\n",
            "Epoch 2/3\n",
            "1250/1250 [==============================] - 509s 407ms/step - loss: 0.4985 - accuracy: 0.1826\n",
            "Epoch 3/3\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4828 - accuracy: 0.1850\n",
            "Epoch 3: saving model to training_1/cp.ckpt\n",
            "1250/1250 [==============================] - 586s 468ms/step - loss: 0.4829 - accuracy: 0.1850\n",
            "Epoch 1/3\n",
            "1250/1250 [==============================] - 516s 412ms/step - loss: 0.5078 - accuracy: 0.1827\n",
            "Epoch 2/3\n",
            "1250/1250 [==============================] - 508s 407ms/step - loss: 0.4877 - accuracy: 0.1857\n",
            "Epoch 3/3\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4749 - accuracy: 0.1875\n",
            "Epoch 3: saving model to training_1/cp.ckpt\n",
            "1250/1250 [==============================] - 584s 467ms/step - loss: 0.4751 - accuracy: 0.1875\n",
            "Epoch 1/3\n",
            "1250/1250 [==============================] - 507s 406ms/step - loss: 0.5057 - accuracy: 0.1812\n",
            "Epoch 2/3\n",
            "1250/1250 [==============================] - 506s 404ms/step - loss: 0.4865 - accuracy: 0.1842\n",
            "Epoch 3/3\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4726 - accuracy: 0.1863\n",
            "Epoch 3: saving model to training_1/cp.ckpt\n",
            "1250/1250 [==============================] - 572s 458ms/step - loss: 0.4725 - accuracy: 0.1863\n",
            "Epoch 1/3\n",
            "1250/1250 [==============================] - 504s 403ms/step - loss: 0.5183 - accuracy: 0.1725\n",
            "Epoch 2/3\n",
            "1250/1250 [==============================] - 513s 411ms/step - loss: 0.4897 - accuracy: 0.1770\n",
            "Epoch 3/3\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4735 - accuracy: 0.1800\n",
            "Epoch 3: saving model to training_1/cp.ckpt\n",
            "1250/1250 [==============================] - 571s 457ms/step - loss: 0.4734 - accuracy: 0.1799\n"
          ]
        }
      ],
      "source": [
        "optimizer, loss = get_optimizer_and_loss()\n",
        "\n",
        "lora_model.compile(optimizer=optimizer,\n",
        "                   loss=loss,\n",
        "                   metrics=[\"accuracy\"])\n",
        "\n",
        "checkpoint_path = \"training_1/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "print(checkpoint_path)\n",
        "\n",
        "\n",
        "# Create a callback that saves the model's weights every 5 epochs\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    save_freq=3*SUBSET_SIZE)\n",
        "\n",
        "\n",
        "\n",
        "lora_model.save_weights(checkpoint_path.format(epoch=0))\n",
        "\n",
        "for subset in health_ds:\n",
        "# Train the model with the new callback\n",
        "  lora_model.fit(subset,\n",
        "            callbacks=[cp_callback],\n",
        "            epochs=3)  # Pass callback to training\n",
        "\n",
        "# This may generate warnings related to saving the state of the optimizer.\n",
        "# These warnings (and similar warnings throughout this notebook)\n",
        "# are in place to discourage outdated usage, and can be ignored.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(checkpoint_dir)"
      ],
      "metadata": {
        "id": "lFaM6AJKwVNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273a1faf-915d-4b12-9ae1-635fd1d74273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cp.ckpt.index', 'cp.ckpt.data-00000-of-00001', 'checkpoint']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting Google Drive in Google Colab for File Access"
      ],
      "metadata": {
        "id": "yfLAMdsF3gBD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6FClRg0txCQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ebb162b-b684-4bca-ae65-170cf6fcaa85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving GPT-2 Model Weights to Google Drive\n",
        "\n"
      ],
      "metadata": {
        "id": "tPTf7TzT3nir"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ke7RiLeDBrp3"
      },
      "outputs": [],
      "source": [
        "lora_model.save_weights('./drive/MyDrive/checkpoints/my_checkpoint')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model with the second part of the database"
      ],
      "metadata": {
        "id": "nJvB3tQO4bz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health_ds = split_dataframe_to_datasets(health_df,2)\n",
        "\n",
        "optimizer, loss = get_optimizer_and_loss()\n",
        "\n",
        "lora_model.compile(optimizer=optimizer,\n",
        "                   loss=loss,\n",
        "                   metrics=[\"accuracy\"])\n",
        "\n",
        "checkpoint_path = \"training_1/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "print(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights every 5 epochs\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    save_freq=4*SUBSET_SIZE)\n",
        "\n",
        "\n",
        "\n",
        "lora_model.save_weights(checkpoint_path.format(epoch=0))\n",
        "\n",
        "for subset in health_ds:\n",
        "# Train the model with the new callback\n",
        "  lora_model.fit(subset,\n",
        "            callbacks=[cp_callback],\n",
        "            epochs=3)  # Pass callback to training\n",
        "\n",
        "lora_model.save_weights('./drive/MyDrive/checkpoints/my_checkpoint')"
      ],
      "metadata": {
        "id": "G7DUpuzO4snS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Test"
      ],
      "metadata": {
        "id": "Zt7IDkzn5wmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    GPT2_PRESET,\n",
        "    sequence_length=128,\n",
        ")\n",
        "model_test = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "    GPT2_PRESET,\n",
        "    preprocessor=preprocessor,\n",
        ")"
      ],
      "metadata": {
        "id": "tN-6eNLW-eGd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aee7ed6-7e2d-447b-8cd0-70dc9281b67e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_large_en/2/download/tokenizer.json...\n",
            "100%|██████████| 448/448 [00:00<00:00, 1.04MB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_large_en/2/download/assets/tokenizer/merges.txt...\n",
            "100%|██████████| 446k/446k [00:00<00:00, 3.82MB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_large_en/2/download/assets/tokenizer/vocabulary.json...\n",
            "100%|██████████| 0.99M/0.99M [00:00<00:00, 6.89MB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_large_en/2/download/config.json...\n",
            "100%|██████████| 485/485 [00:00<00:00, 748kB/s]\n",
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gpt2/keras/gpt2_large_en/2/download/model.weights.h5...\n",
            "100%|██████████| 2.88G/2.88G [01:12<00:00, 42.7MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:37: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/backbone.py:37: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Latest Checkpointed Weights into the Test Model"
      ],
      "metadata": {
        "id": "aMOCoM2Y6KOK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ryz-DCxhg3Ou"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "latest\n",
        "\n",
        "model_test.load_weights(latest)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSf6fRO25NKX"
      },
      "outputs": [],
      "source": [
        "lora_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_test.load_weights('./drive/MyDrive/checkpoints/my_checkpoint')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJK11buW9F_k",
        "outputId": "78d5b220-3ad4-4804-af3b-b68863f95984"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/task.py:47: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "/usr/local/lib/python3.10/dist-packages/keras_nlp/src/models/task.py:47: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x78861784b5b0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating Text with the Test Model: Answering a Medical Question about Glaucoma Symptoms"
      ],
      "metadata": {
        "id": "pP7pnFS86Sey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\n",
        "    model_test, \"The doctor's answer to the question 'What are the symptoms of Glaucoma ?' would be : \", max_length=MAX_GENERATION_LENGTH\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnKA8AT_hUYl",
        "outputId": "a0d8c638-545f-475f-82a1-bb93b57924c7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output:\n",
            "The doctor's answer to the question 'What are the symptoms of Glaucoma ?' would be :  \"Glaucoma is a condition in which the optic nerve is injured by a tumor or other obstruction. The symptoms of glaucoma may include: blurred vision, double vision, double vision, double vision, double vision, or other visual disturbances. The symptoms of glaucoma can be relieved by surgery, medication, or both.\" The doctor's answer to the question 'What are the symptoms of cataracts ?' would be :  \"Cataracts are the result of an injury to the eye. Cataracts are usually caused by a tumor or other obstruction that has been causing damage to the retina. The symptoms of cataracts are: cloudy vision, double vision, blurred vision, double vision, double vision, or other visual disturbances. Cataracts can be treated by surgery, medication, or both.\" The doctor's answer to\n",
            "Total Time Elapsed: 2.96s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\n",
        "    model_test, \"The doctor's answer to the question 'what are the symptoms of asthma? ?' would be : \", max_length=MAX_GENERATION_LENGTH\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaUxU71Thx4d",
        "outputId": "20d8d0c1-b797-46fa-d4f4-884bdfe8af2e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output:\n",
            "The doctor's answer to the question 'what are the symptoms of asthma? ?' would be :                \n",
            "The main symptoms of asthma, such as wheezing and chest pain, are usually mild. Other common symptoms include:               \n",
            "Chest discomfort and tightness in the chest\n",
            "                   \n",
            "Nausea\n",
            "             \n",
            "Total Time Elapsed: 1.95s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\n",
        "    model_test, \"The doctor's answer to the question 'what are the symptoms of Ebola ?' would be : \", max_length=MAX_GENERATION_LENGTH\n",
        ")"
      ],
      "metadata": {
        "id": "CwUJvzaPcZur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\n",
        "    model_test, \"The doctor's answer to the question 'what are the risks of obesity ?' would be : \", max_length=MAX_GENERATION_LENGTH\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teHOprveA-k-",
        "outputId": "c818d187-205a-400a-8262-97ac32349c26"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output:\n",
            "The doctor's answer to the question 'what are the risks of obesity ?' would be :  'The risk of developing diabetes, heart disease and cancer is high.'\n",
            "The risk of developing obesity is high and there are many other factors that could be contributing to this.\n",
            "The risk of obesity is high because it is associated with a high prevalence of obesity, and it is a major cause of chronic diseases. \n",
            "The risk of obesity is not just related to the number of pounds you have. It is related to the number of calories you consume, the amount you exercise, the amount of physical activity you do, the amount of fat you have on your body, and the amount of fat you burn when you eat.\n",
            "The amount of fat you have on your body is also related to your BMI.\n",
            "The risk of obesity is high because of the high prevalence of obesity, and it is a major cause of the chronic diseases that are prevalent in our society. The number of\n",
            "Total Time Elapsed: 47.40s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
